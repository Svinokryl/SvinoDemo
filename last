Скрипт для машины:
#cloud-config
chpasswd:
  expire: false
  users:
  - {name: altlinux, password: P@ssw0rd, type: text}
  - {name: root, password: toor, type: text}
  ssh_pwauth: false

users:
  - name: altlinux
    sudo: ALL=(ALL) NOPASSWD:ALL
    groups: wheel
    shell: /bin/bash
    ssh_authorized_keys:

Сама машина:

sudo apt-get update && sudo apt-get install -y wget unzip
wget https://hashicorp-releases.mcs.mail.ru/terraform/1.14.0/terraform_1.14.0_linux_amd64.zip
sudo unzip  terraform_1.14.0_linux_amd64.zip -d /usr/local/bin/
sudo apt-get install -y python3-module-openstackclient python3-module-pipvim
pip3 install ansible
export PATH=/home/altlinux/.local/bin:$PATH
mkdir -p /home/altlinux/Projects/Project_01
mkdir /home/altlinux/Projects/Project_01/terraform
cd /home/altlinux/Projects/
vim cloudinit.conf
export OS_AUTH_URL=https://172.21.10.79:5000/v3
export OS_IDENTITY_API_VERSION=3
export OS_AUTH_TYPE=password
export OS_PROJECT_DOMAIN_NAME=ModuleG
export OS_USER_DOMAIN_NAME=ModuleG
export OS_PROJECT_NAME=Competitror_prjct_04
export OS_USERNAME=Competitor_04
export OS_PASSWORD=Password4

source cloudinit.conf
openstack --insecure server list:
openstack --insecure network list:
.terraformrc

 provider_installation {
    network_mirror {
        url = "https://terraform-mirror.mcs.mail.ru"
        include = ["registry.terraform.io/*/*"]
    }
    direct {
        exclude = ["registry.terraform.io/*/*"]
    }
}

cd Project_01/terraform/
vim provider.tf
terraform {
  required_providers {
    openstack = {
      source  = "terraform-provider-openstack/openstack"
      version = "2.1.0"
    }
  }
}

provider "openstack" {
  auth_url    = "https://172.21.10.79:5000/v3" 
  tenant_name = "Competitror_prjct_04"
  user_name   = "Competitor_04"
  password    = "Password4"
  insecure    = true
}

terraform init
ssh-keygen -t rsa
cat ~/.ssh/id_rsa.pub > cloud-init.yml
vim cloud-init.yml
#cloud-config
chpasswd:
  expire: false
  users:
  - {name: altlinux, password: P@ssw0rd, type: text}
  - {name: root, password: toor, type: text}
  ssh_pwauth: false

users:
  - name: altlinux
    sudo: ALL=(ALL) NOPASSWD:ALL
    groups: wheel
    shell: /bin/bash
    ssh_authorized_keys:
(добавим ключ который сделали ранее)

vim vm-game.tf
resource "openstack_compute_instance_v2" "game" {
  count     = "3"
  name      = "game0${count.index + 1}"
  flavor_id = "03bf1b85-2f5f-4ada-a07b-8b994b6dcb57"
  user_data = file("cloud-init.yml")

  block_device {
    uuid                  = "827e08fa-fd3c-41cd-92ca-845bb5018478"
    source_type           = "image"
    volume_size           = "10"
    boot_index            = 0
    destination_type      = "volume"
    delete_on_termination = true
  }

  network {
    port = openstack_networking_port_v2.port_vm_game[count.index].id
  }
}

vim vm-haproxy01.tf 
(значение для параметра flavor_id можно узнать используя команду openstack --insecure flavor list скопировав ID Типа ВМ с 1024 RAM и 1 VCPUs;
значение для параметра uuid можно узнать используя команду openstack --insecure image list скопировав ID образа alt-p11-cloud-x86_64.qcow2.)
resource "openstack_compute_instance_v2" "haproxy" {
  name            = "haproxy01"
  flavor_id       = "03bf1b85-2f5f-4ada-a07b-8b994b6dcb57"
  user_data       = file("cloud-init.yml")

  block_device {
    uuid                  = "827e08fa-fd3c-41cd-92ca-845bb5018478"
    source_type           = "image"
    volume_size           = "10"
    boot_index            = 0
    destination_type      = "volume"
    delete_on_termination = true
  }

  network {
    port = openstack_networking_port_v2.haproxy.id
  }
}

vim network.tf
(значение для параметра network_id можно узнать используя команду openstack --insecure network list скопировав ID виртуальной сети с именем cloud-net;
значение для параметра subnet_id можно узнать используя команду openstack --insecure subnet list скопировав ID виртуальной подсети 192.168.1.0/24;)
resource "openstack_networking_port_v2" "port_vm_game" {
    count          = "3"
    name           = "port_vm_game0${count.index + 1}"
    network_id     = "61845892-f9cc-4fde-962c-34b59425a74d"
    admin_state_up = true

    fixed_ip {
        subnet_id   = "13592ca4-8782-410b-9bcc-90810ccab6fe"
        ip_address  = "192.168.1.10${count.index + 1}"
    }
}

resource "openstack_networking_port_v2" "haproxy" {
    name           = "port_vm_haproxy"
    network_id     = "61845892-f9cc-4fde-962c-34b59425a74d"
    admin_state_up = true

    fixed_ip {
        subnet_id   = "13592ca4-8782-410b-9bcc-90810ccab6fe"
        ip_address  = "192.168.1.100"
    }
}

vim floatingip.tf
resource "openstack_networking_floatingip_v2" "floatingip_haproxy" {
  pool = "public"
}

resource "openstack_networking_floatingip_associate_v2" "association_haproxy" {
  port_id     = openstack_networking_port_v2.haproxy.id
  floating_ip = openstack_networking_floatingip_v2.floatingip_haproxy.address
}

terraform validate
cd /home/altlinux/Projects/Project_01
vim  deploy_project_01.sh
#!/bin/bash

cd /home/$USER/Projects
source cloudinit.conf
cd /home/$USER/Projects/Project_01/terraform
terraform init
terraform apply -auto-approve

sleep 60
echo "done"

chmod +x deploy_project_01.sh
./deploy_project_01.sh

vim /etc/hosts
  192.168.32.100  haproxy01.dev.au.team  haproxy01
  192.168.32.100  game.au.team
  192.168.32.101  game01.dev.au.team     game01
  192.168.32.102  game02.dev.au.team     game02
  192.168.32.103  game03.dev.au.team     game03
  192.168.32.104  acm-server.au.team     acm-server
  192.168.32.1-5  db-server.au.team      db-server
  192.168.32.106  bar-agent01.au.team    bar-agent01
  192.168.32.104  cb.au.team
  192.168.32.107  master01.au.team       master01
  192.168.32.108  worker01.au.team       worker01
  192.168.32.109  worker02.au.team       worker02
  192.168.32.107  school-site.au.team    

mkdir /home/altlinux/Projects/Project_01/ansible
cd /home/altlinux/Projects/Project_01/ansible
vim ansible.cfg
[defaults]
inventory = ./inventory.yml
host_key_checking = False
callback_enabled = profile_tasks
callback_whitelist = profile_tasks

vim  inventory.yml
all:
  children:
    proxys:
      hosts:
        haproxy01:

    games:
      hosts:
        game01:
        game02:
        game03:

mkdir group_vars
vim all.yaml
---
ansible_python_interpreter: /usr/bin/python3
ansible_ssh_user: altlinux
ansible_ssh_private_key_file: ~/.ssh/id_rsa

ansible -m ping all

Скачиваем любым способом файлы https://disk.yandex.ru/d/uhpN6U6UYRK_Nw (у приложения есть README от программиста-разработчика)
wget https://disk.yandex.ru/d/uhpN6U6UYRK_Nw -O ~/Project_01.zip
unzip ~/Project_01.zip -d /home/altlinux/Projects/Project_01/2048-game/
vim /home/altlinux/Projects/Project_01/ansible/Dockerfile
FROM node:16-alpine AS builder

WORKDIR /2048-game

COPY package*.json ./
RUN npm install --include=dev

COPY . .
RUN npm run build

EXPOSE 8080

FROM nginx:stable-alpine3.19
COPY --from=builder /2048-game/dist /usr/share/nginx/html

vim  games_playbook.yml: (та же директория)
---
- hosts: games
  become: true

  tasks:
    - name: Install docker
      community.general.apt_rpm:
        name: 
          - docker-engine
          - docker-buildx
        state: present
        update_cache: true

    - name: Started and enabled docker
      ansible.builtin.systemd:
        name: docker
        state: started
        enabled: true

    - name: Copying the project files
      ansible.builtin.copy:
        src: ../2048-game/
        dest: "/home/{{ ansible_ssh_user }}/2048-game/"

    - name: Copying the Dockerfile
      ansible.builtin.copy:
        src: ./Dockerfile
        dest: "/home/{{ ansible_ssh_user }}/2048-game/"

    - name: Build docker image
      community.docker.docker_image_build:
        name: "2048-game"
        tag: latest
        path: "/home/{{ ansible_ssh_user }}/2048-game/"
        dockerfile: Dockerfile

    - name: Start docker container
      community.docker.docker_container:
        name: "2048-game"
        image: "2048-game"
        ports: "80:80"
        state: started
        restart: true

ansible-playbook games_playbook.yml
mkdir files (директория энсибл)
vim haproxy.cfg
global
    log /dev/log daemon
    chroot      /var/lib/haproxy
    pidfile     /run/haproxy.pid
    maxconn     4000
    user        _haproxy
    group       _haproxy
    daemon
    stats socket /var/lib/haproxy/stats

defaults
    mode                    http
    log                     global
    option                  httplog
    option                  dontlognull
    option http-server-close
    option forwardfor       except 127.0.0.0/8
    option                  redispatch
    retries                 3
    timeout http-request    10s
    timeout queue           1m
    timeout connect         10s
    timeout client          1m
    timeout server          1m
    timeout http-keep-alive 10s
    timeout check           10s
    maxconn                 3000

frontend http_front
    bind *:80
    bind *:443 ssl crt /var/lib/ssl/game.pem
    default_backend http_back

backend http_back
    balance roundrobin
    server game01 192.168.1.101:80 check
    server game02 192.168.1.102:80 check
    server game03 192.168.1.103:80 check

listen stats
bind :80
bind *:443 ssl crt /var/lib/ssl/game.pem
stats enable
stats uri /
stats refresh 5s
stats realm Haproxy\ Stats

openssl req -x509 -sha256 -days 3653 -newkey rsa:2048 -keyout ca.key -out ca.crt
  Orga name - au.team
  Common name - ca.au.team
sudo cp ca.crt /etc/pki/ca-trust/source/anchors/ && sudo update-ca-trust
mv ca.* files/
openssl genrsa -out files/game.key 2048
openssl req -key files/game.key -new -out files/game.csr
  Orga name - au.team
  Common name - game.au.team
cat <<EOF > files/game.ext
authorityKeyIdentifier=keyid,issuer
basicConstraints=CA:FALSE
subjectAltName=@alt_names
[alt_names]
DNS.1=game.au.team
IP.1=192.168.32.100
EOF

openssl x509 -req -CA files/ca.crt -CAkey files/ca.key -in files/game.csr -out files/game.crt -days 365 -CAcreateserial -extfile files/game.ext
cat files/game.key files/game.crt > files/game.pem
В директории /home/altlinux/Projects/Project_01/ansible создаём файл proxys_playbook.yml:
vim proxys_playbook.yml
---
- hosts: proxys
  become: true

  tasks:
    - name: Install HAProxy
      community.general.apt_rpm:
        name: haproxy
        state: present
        update_cache: true
    
    - name: Copy file 'haproxy.cfg'
      ansible.builtin.copy:
        src: files/haproxy.cfg
        dest: /etc/haproxy/haproxy.cfg
      notify:
        - Restarted HAProxy

    - name: Copy certificate for HAProxy
      ansible.builtin.copy:
        src: files/game.pem
        dest: /var/lib/ssl/game.pem
      notify:
        - Restarted HAProxy

    - name: Started and enabled HAProxy
      ansible.builtin.systemd:
        name: haproxy
        state: restarted
        enabled: true

  handlers:
    - name: Restarted HAProxy
      ansible.builtin.systemd:
        name: haproxy
        state: restarted

ansible-playbook proxys_playbook.yml
vim /etc/hosts

https://game.au.team
cd /home/altlinux/Projects/Project_01
vim configure_project_01.sh
#!/bin/bash

export PATH=/home/altlinux/.local/bin:$PATH
cd /home/$USER/Projects/Project_01/ansible
sleep 10
ansible-playbook games_playbook.yml
sleep 5
ansible-playbook proxys_playbook.yml

chmod +x configure_project_01.sh
./configure_project_01.sh
vim  destroy_project_01.sh
#!/bin/bash

cd /home/$USER/Projects
source cloudinit.conf
cd /home/$USER/Projects/Project_01/terraform
terraform destroy -auto-approve
rm -f ~/.ssh/known_hosts

echo "done"

chmod +x destroy_project_01.sh
./destroy_project_01.sh

        (Project 2)
cd ~/Projects/
mkdir -p Project_02/terraform
cd Project_02/terraform
cp ~/Projects/Project_01/terraform/provider.tf ./
terraform init
cp ~/Projects/Project_01/terraform/network.tf ./
vim network.tf
resource "openstack_networking_port_v2" "port_vm_acm-server" {
    name           = "port_acm-server"
    network_id     = "61845892-f9cc-4fde-962c-34b59425a74d"
    admin_state_up = true

    fixed_ip {
        subnet_id   = "13592ca4-8782-410b-9bcc-90810ccab6fe"
        ip_address  = "192.168.32.104"
    }
}

resource "openstack_networking_port_v2" "port_vm_db-server" {
    name           = "port_db-server"
    network_id     = "61845892-f9cc-4fde-962c-34b59425a74d"
    admin_state_up = true

    fixed_ip {
        subnet_id   = "13592ca4-8782-410b-9bcc-90810ccab6fe"
        ip_address  = "192.168.32.105"
    }
}

resource "openstack_networking_port_v2" "port_vm_bar-agent01" {
    name           = "port_bar-agent01"
    network_id     = "61845892-f9cc-4fde-962c-34b59425a74d"
    admin_state_up = true

    fixed_ip {
        subnet_id   = "13592ca4-8782-410b-9bcc-90810ccab6fe"
        ip_address  = "192.168.32.106"
    }

cp ~/Projects/Project_01/terraform/vm-game.tf ./vm.tf
vim vf.tf
resource "openstack_compute_instance_v2" "acm-server" {
  name      = "ACM-Server"
  flavor_id = "101"
  user_data = file("cloud-init.yml")

  block_device {
    uuid                  = "827e08fa-fd3c-41cd-92ca-845bb5018478"
    source_type           = "image"
    volume_size           = "20"
    boot_index            = 0
    destination_type      = "volume"
    delete_on_termination = true
  }

  network {
    port = openstack_networking_port_v2.port_vm_acm-server.id
  }
}

resource "openstack_compute_instance_v2" "db-server" {
  name      = "DB-Server"
  flavor_id = "03bf1b85-2f5f-4ada-a07b-8b994b6dcb57"
  user_data = file("cloud-init.yml")

  block_device {
    uuid                  = "827e08fa-fd3c-41cd-92ca-845bb5018478"
    source_type           = "image"
    volume_size           = "20"
    boot_index            = 0
    destination_type      = "volume"
    delete_on_termination = true
  }

  network {
    port = openstack_networking_port_v2.port_vm_db-server.id
  }
}

resource "openstack_compute_instance_v2" "bar-agent01" {
  name      = "BAR-Agent01"
  flavor_id = "03bf1b85-2f5f-4ada-a07b-8b994b6dcb57"
  user_data = file("cloud-init.yml")

  block_device {
    uuid                  = "827e08fa-fd3c-41cd-92ca-845bb5018478"
    source_type           = "image"
    volume_size           = "10"
    boot_index            = 0
    destination_type      = "volume"
    delete_on_termination = true
  }

  network {
    port = openstack_networking_port_v2.port_vm_bar-agent01.id
  }
}

cp ~/Projects/Project_01/terraform/cloud-init.yml ./
terraform apply
ssh acm-server
ssh db-server
ssh bar-agent01
scp CyberBackup_18_64-bit.x86_64 acm-server:~/
scp CyberBackup_18_64-bit.x86_64 db-server:~/
scp CyberBackup_18_64-bit.x86_64 bar-agent01:~/

DB-server:
apt-get update && apt-get install -y postgresql17-server
/etc/init.d/postgresql initdb
systemctl enable --now postgresql
vim /var/lib/pgsql/data/postgresql.conf:
  listen_addresses = '*'
vim /var/lib/pgsql/data/pg_hba.conf
  host  all  all  0.0.0.0/0  md5
systemctl restart postgresql
createuser -U postgres --superuser --encrypted --pwprompt cyberbackup

ACM-Server:
apt-get update && apt-get dist-upgrade -y && update-kernel -y && apt-get clean && reboot
uname -r 
 apt-get install kernel-source-<x.x> (версия ядра)
apt-get install -y kernel-source-6.12
apt-get install -y kernel-headers-modules-6.12 gcc make kmod-sign
cd /home/altlinux/
chmod +x CyberBackup_18_64-bit.x86_64
./CyberBackup_18_64-bit.x86_64
Enter -> Managament server(*) Agent for Linux (*) Bootable Media Builder (8) -> Использовать PostgreSQL -> Хост: 192.168.32.105 Порт: 5432 Пользователь: cyberbackup Пароль: 

Cloud-ADM:
 http://cb.au.team:9877:
root toor
Overriew -> start trial ->  start

DB-Server:
apt-get update && apt-get dist-upgrade -y && update-kernel -y && apt-get clean && reboot
apt-get install -y kernel-source-6.12
apt-get install -y kernel-headers-modules-6.12 gcc make kmod-sign
cd /home/altlinux/
chmod +x CyberBackup_18_64-bit.x86_64
./CyberBackup_18_64-bit.x86_64
  Agenr for Linux (*) -> Agent for PostgreSQL (*) => Сервер управления: 192.168.32.104 root toor ->

Bar-Agent01:
apt-get update && apt-get dist-upgrade -y && update-kernel -y && apt-get clean && reboot
apt-get install -y kernel-source-6.12
apt-get install -y kernel-headers-modules-6.12 gcc make kmod-sign
cd /home/altlinux/
chmod +x CyberBackup_18_64-bit.x86_64
./CyberBackup_18_64-bit.x86_64
Agent for linux -> 192.168.32.104 root toor

Cloud-ADM:
Devices -> Physical 


          Project 3
Cloud-ADM:
cd ~/Projects/
mkdir -p Project_03/terraform
cd Project_03/terraform
cp ~/Projects/Project_02/terraform/provider.tf ./
cp ~/Projects/Project_02/terraform/cloud-init.yml ./
cp ~/Projects/Project_02/terraform/network.tf ./
cp ~/Projects/Project_02/terraform/vm.tf ./
terraform init
vim network.tf
resource "openstack_networking_port_v2" "port_vm_master01" {
    name           = "port_master01"
    network_id     = "61845892-f9cc-4fde-962c-34b59425a74d"
    admin_state_up = true

    fixed_ip {
        subnet_id   = "13592ca4-8782-410b-9bcc-90810ccab6fe"
        ip_address  = "192.168.32.107"
    }
}

resource "openstack_networking_port_v2" "port_vm_worker01" {
    name           = "port_worker01"
    network_id     = "61845892-f9cc-4fde-962c-34b59425a74d"
    admin_state_up = true

    fixed_ip {
        subnet_id   = "13592ca4-8782-410b-9bcc-90810ccab6fe"
        ip_address  = "192.168.32.108"
    }
}

resource "openstack_networking_port_v2" "port_vm_worker02" {
    name           = "port_worker02"
    network_id     = "61845892-f9cc-4fde-962c-34b59425a74d"
    admin_state_up = true

    fixed_ip {
        subnet_id   = "13592ca4-8782-410b-9bcc-90810ccab6fe"
        ip_address  = "192.168.32.109"
    }
}

vim vm.tf
resource "openstack_compute_instance_v2" "master01" {
  name      = "master01"
  flavor_id = "03bf1b85-2f5f-4ada-a07b-8b994b6dcb57"
  user_data = file("cloud-init.yml")

  block_device {
    uuid                  = "827e08fa-fd3c-41cd-92ca-845bb5018478"
    source_type           = "image"
    volume_size           = "10"
    boot_index            = 0
    destination_type      = "volume"
    delete_on_termination = true
  }

  network {
    port = openstack_networking_port_v2.port_vm_master01.id
  }
}

resource "openstack_compute_instance_v2" "worker01" {
  name      = "worker01"
  flavor_id = "03bf1b85-2f5f-4ada-a07b-8b994b6dcb57"
  user_data = file("cloud-init.yml")

  block_device {
    uuid                  = "827e08fa-fd3c-41cd-92ca-845bb5018478"
    source_type           = "image"
    volume_size           = "10"
    boot_index            = 0
    destination_type      = "volume"
    delete_on_termination = true
  }

  network {
    port = openstack_networking_port_v2.port_vm_worker01.id
  }
}

resource "openstack_compute_instance_v2" "worker02" {
  name      = "worker02"
  flavor_id = "03bf1b85-2f5f-4ada-a07b-8b994b6dcb57"
  user_data = file("cloud-init.yml")

  block_device {
    uuid                  = "827e08fa-fd3c-41cd-92ca-845bb5018478"
    source_type           = "image"
    volume_size           = "10"
    boot_index            = 0
    destination_type      = "volume"
    delete_on_termination = true
  }

  network {
    port = openstack_networking_port_v2.port_vm_worker02.id
  }
}

terraform apply
ssh master01
ssh worker01
ssh worker02

master01, worker01 и worker02:
apt-get update && apt-get install -y docker-engine
systemctl enable --now docker
vim /etc/docker/daemon.json
  live-restore false,
systemctl restart docker

master01:
docker swarm init
(скопировать docker swarm join ==token ****)

worker01 и worker02:
Вставить docker swarm 


master01:
docker node ls
docker service create \
    --name registry \
    --publish published=5000,target=5000 \
    --constraint node.role==worker \
    registry:3
docker service ls:
docker service ps registry

Cloud-ADM:
scp Project03.zip master01:~/


master01:
apt-get install -y unzip docker-compose-v2\
unzip /home/altlinux/Project03.zip -d ./
cd School-site-project-main/
vim Dockerfile
FROM nginx:1.29.3-alpine
COPY . /usr/share/nginx/html

vim compose.yaml 
services:
  app:
    image: 127.0.0.1:5000/app
    build: .
    ports:
      - "80:80"

docker compose up -d
docker image ls
docker compose ps

Проверить работоспособность веб-приложения с Cloud-ADM в браузере:
обращаясь в веб-браузере на IP-адрес любой из нод:
docker compose down --volumes
docker image rm 127.0.0.1:5000/app

Cloud-ADM:
openssl genrsa -out site.key 2048
openssl req -key site.key -new -out site.csr
  Organization name au.team
  Common school-site.au.team
vim site.ext
cat <<EOF > site.ext
authorityKeyIdentifier=keyid,issuer
basicConstraints=CA:FALSE
subjectAltName=@alt_names
[alt_names]
DNS.1=school-site.au.team
IP.1=192.168.1.107
EOF
openssl x509 -req -CA Projects/Project_01/ansible/files/ca.crt -CAkey Projects/Project_01/ansible/files/ca.key -in site.csr -out site.crt -days 365 -CAcreateserial -extfile site.ext
scp site.key master01:~/
scp site.crt master01:~/

master01:
cp /home/altlinux/site.crt ./
cp /home/altlinux/site.key ./
vim custom.conf
server {
    listen       80;
    server_name  school-site.au.team;

    location / {
        root   /usr/share/nginx/html;
        index  index.html index.htm;
    }
}

server {
    listen       443 ssl;
    server_name  school-site.au.team;

    ssl_certificate /etc/nginx/site.crt;
    ssl_certificate_key  /etc/nginx/site.key;

    location / {
        root   /usr/share/nginx/html;
        index  index.html index.htm;
    }
}

vim DockerFile
FROM nginx:1.29.3-alpine
COPY site.crt site.key /etc/nginx/
COPY custom.conf /etc/nginx/conf.d/default.conf
COPY . /usr/share/nginx/html

vim compose.yam
services:
  app:
    image: 127.0.0.1:5000/app
    build: .
    ports:
      - "80:80"
      - "443:443"

docker compose up -d
docker compose down --volumes
docker compose push
docker image rm 127.0.0.1:5000/app
vim compose.yaml
services:
  app:
    image: 127.0.0.1:5000/app
    ports:
      - "80:80"
      - "443:443"
    deploy:
      placement:
        constraints:
          - "node.role==worker"

  redis:
    image: redis:8.4.0-alpine
    deploy:
      placement:
        constraints:
          - "node.role==worker"

  db:
    image: postgres:18.1-alpine3.22
    environment:
      POSTGRES_PASSWORD: "P@ssw0rd"
    deploy:
      placement:
        constraints:
          - "node.role==worker"

docker stack deploy --compose-file compose.yaml school-site-project
docker stack ls:
 docker stack services school-site-project:
docker stack ps school-site-project:
